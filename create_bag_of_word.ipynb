{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2eaf411",
   "metadata": {},
   "source": [
    "BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea082db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  \n",
    "import numpy as np  \n",
    "import random  \n",
    "import string\n",
    "\n",
    "import bs4 as bs  \n",
    "import urllib.request\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dcca289",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m raw_html \u001b[38;5;241m=\u001b[39m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://fossbytes.com/us-army-m1-abrams-is-one-of-the-heaviest-tanks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[0;32m      2\u001b[0m raw_html \u001b[38;5;241m=\u001b[39m raw_html\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      4\u001b[0m article_html \u001b[38;5;241m=\u001b[39m bs\u001b[38;5;241m.\u001b[39mBeautifulSoup(raw_html, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "raw_html = urllib.request.urlopen('https://fossbytes.com/us-army-m1-abrams-is-one-of-the-heaviest-tanks')  \n",
    "raw_html = raw_html.read()\n",
    "\n",
    "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
    "\n",
    "article_paragraphs = article_html.find_all('p')\n",
    "\n",
    "article_text = ''\n",
    "\n",
    "for para in article_paragraphs:  \n",
    "    article_text += para.text\n",
    "    \n",
    "article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ba3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = nltk.sent_tokenize(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebda1a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus )):\n",
    "    corpus [i] = corpus [i].lower()\n",
    "    corpus [i] = re.sub(r'\\W',' ',corpus [i])\n",
    "    corpus [i] = re.sub(r'\\s+',' ',corpus [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102da85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(corpus))\n",
    "print(corpus[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213c1a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = {}\n",
    "list_word=[]\n",
    "for sentence in corpus:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "            list_word.append(token)\n",
    "        else:\n",
    "            wordfreq[token] += 1\n",
    "wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48776c54",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordfreq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mheapq\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m most_freq \u001b[38;5;241m=\u001b[39m heapq\u001b[38;5;241m.\u001b[39mnlargest(\u001b[38;5;241m200\u001b[39m, \u001b[43mwordfreq\u001b[49m, key\u001b[38;5;241m=\u001b[39mwordfreq\u001b[38;5;241m.\u001b[39mget)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wordfreq' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import heapq\n",
    "most_freq = heapq.nlargest(200, wordfreq, key=wordfreq.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30caec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for sentence in corpus:\n",
    "    sentence_tokens = nltk.word_tokenize(sentence)\n",
    "    sent_vec = []\n",
    "    for token in most_freq:\n",
    "        if token in sentence_tokens:\n",
    "            sent_vec.append(1)\n",
    "        else:\n",
    "            sent_vec.append(0)\n",
    "    sentence_vectors.append(sent_vec)\n",
    "sentence_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d2c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = np.asarray(sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d7ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525818db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(list_word, columns = ['col_1'])\n",
    "print(len(df))\n",
    "df=df.drop_duplicates(\n",
    "    subset=None, \n",
    "    keep='first', \n",
    "    inplace=False, \n",
    "    ignore_index=False\n",
    "    )\n",
    "print(len(df))\n",
    "\n",
    "# df.to_csv(r\"C:\\Users\\Kishor Kore\\Desktop\\pandas\\bag_of_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eca5f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\Kishor Kore\\Desktop\\pandas\\bag_of_words\")\n",
    " \n",
    "df.rename(columns = {df.columns[0]:'TEST'}, inplace = True)\n",
    "serach=input(\"enter the serch : \").lower()\n",
    "ds=df[df['col_1'].str.startswith(serach)].index[:10]\n",
    "list_val=[df['col_1'][x] for x in ds]\n",
    "list_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "str1=\"what is e\"\n",
    "from nltk.tokenize import word_tokenize\n",
    "words=word_tokenize(str1)\n",
    "lenw=len(words[-1])\n",
    "lenw\n",
    "str2=str1[:len(str1)-lenw]+\"march\"\n",
    "str_val=[str1[:len(str1)-lenw]+x for x in list_val]\n",
    "str_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d5d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import re\n",
    "import yake\n",
    " \n",
    "def get_nouns_multipartite(content):\n",
    "    kw_extractor = yake.KeywordExtractor(top=100, stopwords=None)\n",
    "    keywords = kw_extractor.extract_keywords(content)\n",
    "    out=[]\n",
    "    for i in range(len(keywords)):\n",
    "        out.append(keywords[i][0])\n",
    "    return out\n",
    "\n",
    "\n",
    "text =  article_text\n",
    "imp_keywords = get_nouns_multipartite(text)\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text=text.replace(\"\\n\",\"\")\n",
    "sline=sent_tokenize(text)\n",
    "print( \"\\n\\n\\n\")\n",
    " \n",
    "    \n",
    "count=1\n",
    "print(imp_keywords)\n",
    "\n",
    "\n",
    "# for line in sline:\n",
    "#     for keyword in imp_keywords:\n",
    "#         match=re.findall(keyword,line,re.IGNORECASE)\n",
    "#         if match:\n",
    "#             keyword=match[0]\n",
    "#         if keyword in line:\n",
    "#            ans=line.replace(keyword,\"______________\")\n",
    "#            print(count,\":  \",ans)\n",
    "#            count+=1\n",
    "#            print(\"_________________________________________________________________________________\\n\\n\")\n",
    "#            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aa84ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pke\n",
    "\n",
    "# taken from the Wikipedia page of Python\n",
    "text = \"\"\"\n",
    "Narendra Damodardas Modi (Gujarati: [ˈnəɾendɾə dɑmodəɾˈdɑs ˈmodiː] (listen); born 17 September 1950)[a] is an Indian \n",
    "politician serving as the 14th and current prime minister of India since 2014. Modi was the chief minister of Gujarat from 2001 to\n",
    " 2014 and is the Member of Parliament from Varanasi. He is a member of the Bharatiya Janata Party (BJP) and of the Rashtriya\n",
    "  Swayamsevak Sangh (RSS), a right-wing Hindu nationalist paramilitary volunteer organisation. He is the first prime minister \n",
    "  to have been born after India's independence in 1947 and the longest serving prime minister from outside the Indian National\n",
    "   Congress.\"\"\"\n",
    "\n",
    "# define the set of valid Part-of-Speeches\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "# 1. create a TextRank extractor.\n",
    "extractor = pke.unsupervised.TextRank()\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=text,\n",
    "                        language='en',\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. build the graph representation of the document and rank the words.\n",
    "#    Keyphrase candidates are composed from the 33-percent\n",
    "#    highest-ranked words.\n",
    "extractor.candidate_weighting(window=2,\n",
    "                              pos=pos,\n",
    "                              top_percent=0.33)\n",
    "\n",
    "# 4. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=3)\n",
    "\n",
    "print(keyphrases)\n",
    "\n",
    "import yake\n",
    "print(\"________________________________________\")\n",
    " \n",
    "kw_extractor = yake.KeywordExtractor(top=10, stopwords=None)\n",
    "keywords = kw_extractor.extract_keywords(text)\n",
    "# for kw, v in keywords:\n",
    "#   print(\"Keyphrase: \",kw, \": score\", v)\n",
    "print(keywords[:])\n",
    "# from yake import KeywordExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cc559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pip\n",
    "# example text\n",
    "text =  article_text\n",
    " \n",
    "import pytextrank\n",
    "\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# examine the top-ranked phrases in the document\n",
    "c=847\n",
    "for phrase in doc._.phrases[:]:\n",
    "    if \",\" in phrase.text or \".\" in phrase.text or \"-\" in phrase.text :\n",
    "        pass\n",
    "    else:\n",
    "        print(str(c)+\"{}\".format(\",\")+phrase.text.lower())\n",
    "        c=c+1\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a939ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff84fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the scrapy module\n",
    "import scrapy\n",
    "\n",
    "class ExtractUrls(scrapy.Spider):\n",
    " name = \"extract\"\n",
    "\n",
    " # request function\n",
    " def start_requests(self):\n",
    "  urls = [ 'https://www.geeksforgeeks.org', ]\n",
    "  \n",
    "  for url in urls:\n",
    "   yield scrapy.Request(url = url, callback = self.parse)\n",
    "\n",
    " # Parse function\n",
    " def parse(self, response):\n",
    "  \n",
    "  # Extra feature to get title\n",
    "  title = response.css('title::text').extract_first()\n",
    "  \n",
    "  # Get anchor tags\n",
    "  links = response.css('a::attr(href)').extract() \n",
    "  \n",
    "  for link in links:\n",
    "   yield\n",
    "   {\n",
    "    'title': title,\n",
    "    'links': link\n",
    "   }\n",
    "   \n",
    "   if 'geeksforgeeks' in link:  \n",
    "    yield scrapy.Request(url = link, callback = self.parse)\n",
    "c=ExtractUrls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9f784d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "22eb5e2c22c238faa93ba91a82f6814d9ac5291abf168a8545504b183eda394b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
